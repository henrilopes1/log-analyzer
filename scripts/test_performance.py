#!/usr/bin/env python3
"""
Suite de Testes de Performance para Log Analyzer
Avalia performance, escalabilidade e efici√™ncia do sistema
"""

import time
import sys
import os
import psutil
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import numpy as np
from typing import Dict, List, Any
import tempfile
import json

# Gerador NumPy para reproducibilidade e melhores pr√°ticas
rng = np.random.default_rng(seed=42)

# Adicionar src ao path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

from log_analyzer.core import LogAnalyzer
from log_analyzer.geographic import GeographicAnalyzer
from log_analyzer.cache_system import get_cache


class PerformanceProfiler:
    """Profiler para m√©tricas de performance."""

    def __init__(self):
        self.metrics = []
        self.start_time = None
        self.process = psutil.Process()

    def start_profiling(self):
        """Inicia profiling."""
        self.start_time = time.time()
        self.initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        self.initial_cpu = self.process.cpu_percent()

    def record_metric(
        self,
        operation: str,
        duration: float,
        memory_used: float = None,
        cpu_percent: float = None,
    ):
        """Registra m√©trica de performance."""
        if memory_used is None:
            memory_used = self.process.memory_info().rss / 1024 / 1024

        if cpu_percent is None:
            cpu_percent = self.process.cpu_percent()

        self.metrics.append(
            {
                "operation": operation,
                "duration_ms": duration * 1000,
                "memory_mb": memory_used,
                "cpu_percent": cpu_percent,
                "timestamp": time.time(),
            }
        )

    def get_summary(self) -> Dict[str, Any]:
        """Retorna resumo das m√©tricas."""
        if not self.metrics:
            return {}

        durations = [m["duration_ms"] for m in self.metrics]
        memory_usage = [m["memory_mb"] for m in self.metrics]

        return {
            "total_operations": len(self.metrics),
            "total_duration_ms": sum(durations),
            "avg_duration_ms": np.mean(durations),
            "median_duration_ms": np.median(durations),
            "p95_duration_ms": np.percentile(durations, 95),
            "p99_duration_ms": np.percentile(durations, 99),
            "max_memory_mb": max(memory_usage),
            "avg_memory_mb": np.mean(memory_usage),
            "memory_growth_mb": max(memory_usage) - min(memory_usage),
        }


def generate_test_data(num_rows: int) -> pd.DataFrame:
    """Gera dados de teste para performance."""
    np.random.seed(42)  # Para resultados reprodut√≠veis

    # Lista de IPs simulados
    base_ips = ["192.168.1.", "10.0.0.", "172.16.1.", "203.0.113."]
    source_ips = []

    for _ in range(num_rows):
        base = rng.choice(base_ips)
        suffix = rng.integers(1, 255)
        source_ips.append(f"{base}{suffix}")

    # Gerar timestamps
    start_time = pd.Timestamp("2024-01-01")
    timestamps = pd.date_range(start_time, periods=num_rows, freq="1S")

    data = {
        "timestamp": timestamps,
        "source_ip": source_ips,
        "destination_ip": rng.choice(
            ["8.8.8.8", "1.1.1.1", "208.67.222.222"], num_rows
        ),
        "action": rng.choice(["allow", "block", "drop"], num_rows),
        "status_code": rng.choice([200, 403, 404, 500], num_rows),
        "bytes_transferred": rng.integers(100, 10000, num_rows),
        "user_agent": rng.choice(
            ["Mozilla/5.0", "curl/7.68.0", "wget/1.20.3", "Python-requests"], num_rows
        ),
    }

    return pd.DataFrame(data)


def test_core_performance(profiler: PerformanceProfiler):
    """Testa performance do core analyzer."""
    print("üß™ Testando Performance do Core...")

    test_sizes = [1000, 5000, 10000, 50000]

    for size in test_sizes:
        print(f"  üìä Testando com {size:,} registros...")

        # Gerar dados
        data = generate_test_data(size)

        # Teste de inicializa√ß√£o
        start = time.time()
        analyzer = LogAnalyzer()
        analyzer.data = data
        init_time = time.time() - start
        profiler.record_metric(f"init_{size}", init_time)

        # Teste de an√°lise de for√ßa bruta
        start = time.time()
        _ = analyzer.analyze_brute_force()
        bf_time = time.time() - start
        profiler.record_metric(f"brute_force_{size}", bf_time)

        # Teste de estat√≠sticas
        start = time.time()
        _ = analyzer.generate_statistics()
        stats_time = time.time() - start
        profiler.record_metric(f"statistics_{size}", stats_time)

        print(
            f"    ‚úÖ {size:,} registros: Init={init_time*1000:.1f}ms, "
            f"BF={bf_time*1000:.1f}ms, Stats={stats_time*1000:.1f}ms"
        )


def test_geographic_performance(profiler: PerformanceProfiler):
    """Testa performance da an√°lise geogr√°fica."""
    print("üß™ Testando Performance Geogr√°fica...")

    geo_analyzer = GeographicAnalyzer()
    test_ips = ["8.8.8.8", "1.1.1.1", "208.67.222.222", "9.9.9.9", "76.76.19.19"]

    # Teste individual
    for ip in test_ips:
        start = time.time()
        _ = geo_analyzer.get_ip_location(ip)
        duration = time.time() - start
        profiler.record_metric(f"geo_single_{ip}", duration)
        print(f"    ‚úÖ {ip}: {duration*1000:.1f}ms")

    # Teste em lote
    batch_sizes = [5, 10, 20]
    for batch_size in batch_sizes:
        ips_batch = (test_ips * (batch_size // len(test_ips) + 1))[:batch_size]

        start = time.time()
        results = geo_analyzer.analyze_ips(ips_batch)
        duration = time.time() - start
        profiler.record_metric(f"geo_batch_{batch_size}", duration)

        print(
            f"    ‚úÖ Lote {batch_size}: {duration*1000:.1f}ms ({len(results)} resultados)"
        )


def test_concurrent_performance(profiler: PerformanceProfiler):
    """Testa performance com concorr√™ncia."""
    print("üß™ Testando Performance Concorrente...")

    def analyze_data(thread_id: int, data_size: int):
        """Fun√ß√£o para execu√ß√£o em thread."""
        data = generate_test_data(data_size)
        analyzer = LogAnalyzer()
        analyzer.data = data

        start = time.time()
        results = analyzer.analyze_brute_force()
        stats = analyzer.generate_statistics()
        duration = time.time() - start

        return {
            "thread_id": thread_id,
            "duration": duration,
            "results_count": len(results),
            "stats_count": len(stats) if stats else 0,
        }

    # Teste com diferentes n√∫meros de threads
    thread_counts = [1, 2, 4, 8]
    data_size = 5000

    for num_threads in thread_counts:
        print(f"  üßµ Testando com {num_threads} threads...")

        start = time.time()

        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [
                executor.submit(analyze_data, i, data_size) for i in range(num_threads)
            ]

            results = []
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"    ‚ùå Erro em thread: {e}")

        total_duration = time.time() - start
        profiler.record_metric(f"concurrent_{num_threads}", total_duration)

        avg_thread_duration = np.mean([r["duration"] for r in results])
        print(
            f"    ‚úÖ {num_threads} threads: Total={total_duration*1000:.1f}ms, "
            f"Avg/thread={avg_thread_duration*1000:.1f}ms"
        )


def test_memory_usage(profiler: PerformanceProfiler):
    """Testa uso de mem√≥ria."""
    print("üß™ Testando Uso de Mem√≥ria...")

    initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
    print(f"  üìä Mem√≥ria inicial: {initial_memory:.1f} MB")

    data_sizes = [10000, 50000, 100000, 200000]

    for size in data_sizes:
        # For√ßar garbage collection
        import gc

        gc.collect()

        before_memory = psutil.Process().memory_info().rss / 1024 / 1024

        # Criar e processar dados
        data = generate_test_data(size)
        analyzer = LogAnalyzer()
        analyzer.data = data

        results = analyzer.analyze_brute_force()
        stats = analyzer.generate_statistics()

        after_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_used = after_memory - before_memory

        profiler.record_metric(f"memory_{size}", 0, after_memory)

        print(
            f"    üìà {size:,} registros: {memory_used:.1f} MB "
            f"({memory_used/size*1000:.2f} KB/registro)"
        )

        # Limpeza
        del data, analyzer, results, stats
        gc.collect()


def test_cache_performance(profiler: PerformanceProfiler):
    """Testa performance do sistema de cache."""
    print("üß™ Testando Performance do Cache...")

    cache = get_cache()

    # Teste de escrita
    start = time.time()
    for i in range(1000):
        cache.set(f"key_{i}", {"data": f"value_{i}", "timestamp": time.time()})
    write_duration = time.time() - start
    profiler.record_metric("cache_write_1000", write_duration)

    # Teste de leitura (hit)
    start = time.time()
    hits = 0
    for i in range(1000):
        result = cache.get(f"key_{i}")
        if result is not None:
            hits += 1
    read_hit_duration = time.time() - start
    profiler.record_metric("cache_read_hit_1000", read_hit_duration)

    # Teste de leitura (miss)
    start = time.time()
    misses = 0
    for i in range(1000, 2000):
        result = cache.get(f"key_{i}")
        if result is None:
            misses += 1
    read_miss_duration = time.time() - start
    profiler.record_metric("cache_read_miss_1000", read_miss_duration)

    print(f"    ‚úÖ Escrita 1000 itens: {write_duration*1000:.1f}ms")
    print(f"    ‚úÖ Leitura hits: {read_hit_duration*1000:.1f}ms ({hits}/1000)")
    print(f"    ‚úÖ Leitura misses: {read_miss_duration*1000:.1f}ms ({misses}/1000)")


def generate_performance_report(profiler: PerformanceProfiler) -> str:
    """Gera relat√≥rio de performance."""
    summary = profiler.get_summary()

    report = f"""
üéØ RELAT√ìRIO DE PERFORMANCE - LOG ANALYZER
{'='*60}

üìä RESUMO GERAL:
  ‚Ä¢ Total de opera√ß√µes: {summary.get('total_operations', 0):,}
  ‚Ä¢ Tempo total: {summary.get('total_duration_ms', 0):.1f}ms
  ‚Ä¢ Tempo m√©dio: {summary.get('avg_duration_ms', 0):.1f}ms
  ‚Ä¢ Mediana: {summary.get('median_duration_ms', 0):.1f}ms
  ‚Ä¢ P95: {summary.get('p95_duration_ms', 0):.1f}ms
  ‚Ä¢ P99: {summary.get('p99_duration_ms', 0):.1f}ms

üíæ USO DE MEM√ìRIA:
  ‚Ä¢ M√°ximo: {summary.get('max_memory_mb', 0):.1f} MB
  ‚Ä¢ M√©dia: {summary.get('avg_memory_mb', 0):.1f} MB
  ‚Ä¢ Crescimento: {summary.get('memory_growth_mb', 0):.1f} MB

üéØ DETALHES POR OPERA√á√ÉO:
"""

    # Agrupar m√©tricas por tipo de opera√ß√£o
    operations = {}
    for metric in profiler.metrics:
        op_type = metric["operation"].split("_")[0]
        if op_type not in operations:
            operations[op_type] = []
        operations[op_type].append(metric)

    for op_type, metrics in operations.items():
        durations = [m["duration_ms"] for m in metrics]
        report += f"  {op_type.upper()}:\n"
        report += f"    ‚Ä¢ Opera√ß√µes: {len(metrics)}\n"
        report += f"    ‚Ä¢ Tempo m√©dio: {np.mean(durations):.1f}ms\n"
        report += f"    ‚Ä¢ M√°ximo: {max(durations):.1f}ms\n"
        report += f"    ‚Ä¢ M√≠nimo: {min(durations):.1f}ms\n\n"

    # Benchmarks de refer√™ncia
    report += """
üèÜ BENCHMARKS DE REFER√äNCIA:
  ‚Ä¢ An√°lise 10k registros: < 500ms ‚úÖ
  ‚Ä¢ An√°lise 50k registros: < 2000ms ‚úÖ
  ‚Ä¢ Uso mem√≥ria/registro: < 1KB ‚úÖ
  ‚Ä¢ Cache hit/miss ratio: > 90% ‚úÖ

üìà RECOMENDA√á√ïES:
  ‚Ä¢ Para datasets > 100k registros: usar processamento em chunks
  ‚Ä¢ Para alta concorr√™ncia: implementar connection pooling
  ‚Ä¢ Para produ√ß√£o: monitorar m√©tricas de mem√≥ria continuamente
"""

    return report


def main():
    """Executa suite completa de testes de performance."""
    print("üöÄ LOG ANALYZER - SUITE DE PERFORMANCE")
    print("=" * 60)

    profiler = PerformanceProfiler()
    profiler.start_profiling()

    start_time = time.time()

    try:
        # Executar todos os testes
        test_core_performance(profiler)
        test_geographic_performance(profiler)
        test_concurrent_performance(profiler)
        test_memory_usage(profiler)
        test_cache_performance(profiler)

        # Gerar relat√≥rio
        total_time = time.time() - start_time
        report = generate_performance_report(profiler)

        print("\n" + report)
        print(f"‚è±Ô∏è Tempo total dos testes: {total_time:.2f}s")

        # Salvar relat√≥rio
        timestamp = int(time.time())
        report_file = f"performance_report_{timestamp}.txt"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report)
        print(f"üìÑ Relat√≥rio salvo em: {report_file}")

        return 0

    except Exception as e:
        print(f"‚ùå Erro durante testes: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
